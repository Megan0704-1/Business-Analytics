---
title: "HW11"
author: '108048110'
date: "4/27/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# BACS HW - Week 11

------------------------------------------------------------------------

## Prerequisite

```{r message=FALSE, warning=FALSE}
library(car)
library(ggplot2)
library(corrplot)
library(tidyverse)
require(gridExtra)

theme_set(theme_bw(base_size=16))
```

```{r message=FALSE, warning=FALSE}
auto <- read.table("auto-data.txt", header=FALSE, na.strings = "?")
names(auto) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", 
                 "acceleration", "model_year", "origin", "car_name")
auto = auto[complete.cases(auto),]

# return regression model
LMOfCars <- function(data){
  lm(mpg~cylinders+
             displacement+
             horsepower+
             weight+
             acceleration+
             auto$model_year+
             factor(origin), 
             data = data,
             na.action = na.exclude)
}
```

------------------------------------------------------------------------

## Observations from full regression model

### 1. Only weight, year, and origin had significant effects.

```{r warning=FALSE}
summary(LMOfCars(auto))
```

### 2. Non-significant factors were highly correlated with weight.

```{r message=FALSE, warning=FALSE}
cor_plt <- function(data){
  cor_data <- round(cor(data[, 1:8], use='pairwise.complete.obs'), 3)
  cor_sorted_data <- names(sort(cor_data[, 'mpg'], decreasing = TRUE))
  cor_data <- cor_data[cor_sorted_data, cor_sorted_data]
  
  corrplot.mixed(cor_data, tl.col='black', tl.pos='lt')
}

cor_plt(auto)
```

### 3. Displacement has the opposite effect in the regression from its visualized effect.

**Note.** Visualization is always right.

```{r message=FALSE, warning=FALSE}
summary(LMOfCars(auto))

plt <- function(a, b, title=''){
  ggplot(auto, aes(x=a, y=b))+
    geom_point()+
    facet_wrap(~factor(origin))+
    stat_smooth(method=lm)+
    labs(x='', y='')+
    ggtitle(title)
}

plt(auto$mpg, auto$displacement, 'mpg v.s. displacement')

```

-   Displacement in regression: -0.02398

### 4. Factors like horsepower and weight, seem to have a nonlinear relationship with mpg

```{r message=FALSE, warning=FALSE}
plt(auto$mpg, auto$horsepower, 'mpg v.s. horsepower')
plt(auto$mpg, auto$weight, 'mpg v.s. weight')
plt(auto$mpg, auto$acceleration, 'mpg v.s. acceleration')
```

------------------------------------------------------------------------

## Question 1) Nonlinearity

```{r message=FALSE, warning=FALSE}
cars <- auto
cars_log <- with(cars, data.frame(log(mpg),
                                  log(cylinders), 
                                  log(displacement),
                                  log(horsepower),
                                  log(weight),
                                  log(acceleration),
                                  model_year,
                                  origin))            

names(cars_log) <- c("mpg", "cylinders", "displacement", "horsepower", "weight", 
                 "acceleration", "model_year", "origin")

knitr::kable(head(cars_log))
```

> ### a. Run a new regression on the `cars_log` dataset.
>
> -   ***i.*** Which log-transformed factors have a significant effect on `log(mpg)` at 10% significance?
>
>     ```{r}
>     summary(LMOfCars(cars_log))
>     ```
>
>     -   **Ans.** horsepower, weight, model_year
>
> -   ***ii.*** Do some new factors now have effects on `mpg`? Why?
>
>     -   **Ans.** Yes, new factors like horsepower, weight, have effects on mpg.
>
>     -   we are able to get linear relationships by applying log transformation on every values in the dataset.
>
> -   ***iii.*** Which factors still have insignificant effects on mpg?\
>     Why?
>
>     ```{r}
>     cor_plt(cars_log)
>     sort(cor(cars_log)[, 'mpg'], decreasing=TRUE)
>     ```
>
>     -   **Ans.** Cylinders and displacement still have insignificant effects on mpg.
>
>     -   This might be result from the nonlinearity present in the data.
>
> ### b. Take a closer look at weight.
>
> -   ***i.*** Create a regression of `mpg` over `weight` from the original `cars` dataset.
>
>     ```{r message=FALSE, warning=FALSE}
>     regr_cars <- summary(LMOfCars(cars))
>     regr_wt <- lm(mpg~weight, data=cars, na.action=na.exclude)
>     regr_wt$coefficients
>     ```
>
> -   ***ii.*** Create a regression of `log(mpg)` on `log(weight)` from `cars_log`.
>
>     ```{r message=FALSE, warning=FALSE}
>     regr_cars_log <- summary(LMOfCars(cars_log))
>     regr_wt_log <- lm(mpg~weight, data=cars_log, na.action=na.exclude)
>     regr_wt_log$coefficients
>     ```
>
> -   ***iii.*** Visualize the residuals of both regression models (raw and log-transformed):
>
>     ```{r message=FALSE, warning=FALSE}
>     tibble_density_plot <- function(data1, data2, title='', t1='', t2=''){
>       ToDF <- function(x, name=''){
>         temp <- list()
>         temp$residuals <- x$residuals
>         temp$from <- rep(name, length(x$residuals))
>         return(data.frame(temp))
>       }
>       
>       d1 <- ToDF(data1, t1)
>       d2 <- ToDF(data2, t2)
>       temp <- as.tibble(rbind(d1, d2))
>       
>       temp %>%
>         ggplot(aes(x=residuals, fill=from))+
>         geom_density(alpha=0.5)+
>         geom_vline(xintercept = c(mean(d1$residuals),
>                                   mean(d2$residuals)),
>                    col=c('red', 'green'))+
>         labs(x='residuals',
>              subtitle=title,
>              caption='source: auto-data.txt')+
>         theme(legend.position = 'bottom')
>     }
>     ```
>
>     -   <div>
>
>         1.  Density plots of residuals.
>
>             ```{r message=FALSE, warning=FALSE}
>             tibble_density_plot(regr_cars,
>                                 regr_cars_log, 
>                                 'Cars',
>                                 'cars',
>                                 'log cars')
>             tibble_density_plot(regr_wt,
>                                 regr_wt_log, 
>                                 'Weight',
>                                 'weights',
>                                 'log weights')
>             tibble_density_plot(regr_wt,
>                                 regr_cars,
>                                 'Weight v.s. Cars',
>                                 'weights',
>                                 'cars')
>             tibble_density_plot(regr_wt_log,
>                                 regr_cars_log,
>                                 '(Logged) Weight v.s. Cars',
>                                 'log weight',
>                                 'log cars')
>             ```
>
>         2.  Scatterplot of `log(weight)` vs. `residuals`
>
>             ```{r message=FALSE, warning=FALSE}
>             p1 <- ggplot()+
>               aes(cars$mpg, resid(regr_wt))+
>               geom_point(col="red")+
>               stat_smooth(method=lm)+
>               geom_hline(yintercept=0, lty='dashed')+
>               labs(x='weight', y='residuals')+
>               ggtitle('mpg v.s. weight')
>             >
>             p2 <- ggplot()+
>               aes(cars_log$mpg, resid(regr_wt_log))+
>               geom_point(col="red")+
>               stat_smooth(method=lm)+
>               geom_hline(yintercept=0, lty='dashed')+
>               labs(x='log weight', y='residuals')+
>               ggtitle('log.mpg v.s. log.weight')
>             >
>             grid.arrange(p1, p2, ncol=2)
>             ```
>
>         </div>
>
> -   ***iv.*** Which regression produces better distributed residuals for the assumptions of regression?
>
> +--------------------------------------------------------------------------+-----------------------------------------------------------+
> | Requirements                                                             | Implications                                              |
> +==========================================================================+===========================================================+
> | 1.  random, normally distributed error terms, with mean(${\epsilon}$)=0. | values of ${\bar y}$ are on the regression line.          |
> |                                                                          |                                                           |
> |                                                                          | ${\hat \beta}$ are symmetrically distributed.             |
> +--------------------------------------------------------------------------+-----------------------------------------------------------+
> | 2.  $var ({\epsilon})$ is the same for all values of $x$.                | The distribution of $y$ is the same across values of $x$. |
> +--------------------------------------------------------------------------+-----------------------------------------------------------+
> | 3.  ${\epsilon}$ are independent.                                        | The values of $y$ are independent.                        |
> +--------------------------------------------------------------------------+-----------------------------------------------------------+
>
> : Regression Assumptions
>
> **Ans.** As we can observe from the plots above, the log-transformed regression produces a better distributed residuals that looks less likely to a curve.
>
> -   ***v.*** Interpret the slope of `log(weight)` vs `log(mpg)` in simple words.
>
>     ```{r}
>     summary(regr_wt)
>     summary(regr_wt_log)
>     ```
>
>     -   **Ans.** The slope of `log(mpg)` vs. `log(weight)` is much flatter then the `mpg` vs. `weight` due to the log transformation.
>
> ### c. Examine the 95% confidence interval of the slope of `log(weight)` vs. `log(mpg)`.
>
> -   ***i.*** Create a bootstrapped confidence interval.
>
>     ```{r message=FALSE, warning=FALSE}
>     # empty canvas
>     plot(log(cars$weight), 
>          log(cars$mpg),
>          type="n", 
>          xlab='weight',
>          ylab='mpg', 
>          main='Bootstrapped Confidence Interval')
>     # function for single resampleed reggresion line
>     points(log(cars$weight), log(cars$mpg), col="skyblue", pch=19)
>     >
>     boot_regr <- function(model, dataset){
>       # random row index number
>       boot_index <- sample(1:nrow(dataset), replace=TRUE) 
>       data_boot <- dataset[boot_index, ] # picking the rows
>       regr_boot <- lm(model, data=data_boot) # run regression model
>       abline(regr_boot, lwd=1, col=rgb(0.7, 0.3, 0.3, 0.2), alpha=0.05)
>       regr_boot$coefficients
>     }
>     coeffs <- replicate(3000, boot_regr(log(mpg)~log(weight), cars))
>     >
>     abline(a = mean(coeffs["(Intercept)", ]),
>            b = mean(coeffs["log(weight)", ]),
>            lwd=2)
>     >
>     # Confidence interval values
>     round(quantile(coeffs["log(weight)", ], c(0.025, 0.975)), 3)
>     ```
>
> -   ***ii.*** Verify your results with a confidence interval using traditional statistics.
>
>     ```{r message=FALSE, warning=FALSE}
>     # traditional way
>     regression_model <- lm(log(mpg)~log(weight), data=cars)
>     summary(regression_model)
>     # estimate +- 1.96*stderr
>     round(regression_model$coefficients['log(weight)']+c(-1.96, 1.96)*0.0297, 3)
>     round(confint(regression_model,'log(weight)', level=0.95), 3)
>     >
>     ```
>
>     -   **Note.** Slightly different results were presented with different computing methods.
>
>     -   When your are not having a decent dataset, bootstrapping will be a more reliable way to compute the regression.
>
> ```{r message=FALSE, warning=FALSE}
> ggplot()+
>   aes(coeffs['log(weight)', ])+
>   geom_density(fill='skyblue', alpha=0.5)+
>   geom_vline(xintercept=mean(coeffs['log(weight)',]), lty='dashed')+
>   geom_vline(xintercept=quantile(coeffs['log(weight)', ],c(0.025, 0.975)), 
>              col=2,
>              lwd=1.5, 
>              lty='dotted')+
>   labs(x='log.weight', subtitle = 'Traditional Stat results')
> ```
>
> **Note.** By finding the slope we get an estimate of the slope by which the dependent variable(mpg) is expected to increase or decrease.
>
> The Confident interval provides the range of the slope values that we expect 95% of the times when the sample size is the same.
>
> Since obviously neither of the two results includes 0 in their confident interval, we can conclude that there is a significant linear relationship between weight and mpg.

------------------------------------------------------------------------

## Question 2) Multicollinearity

```{r message=FALSE, warning=FALSE}
regr_log <- LMOfCars(cars_log)
# diagnosing multicollinearity

weight_regr <- lm(weight~
                    cylinders+
                    displacement+
                    horsepower+
                    acceleration+
                    cars$model_year+
                    factor(origin), 
                  data=cars_log, 
                  na.action = na.exclude)

r2_log_weight <- summary(weight_regr)$r.squared
r2_log_weight
```

> ### a. Compute the VIF of `log(weight)`.
>
> ```{r}
> vif_log_weight <- 1/(1-r2_log_weight)
> vif_log_weight
> >
> ```
>
> -   Multicollinearity inflates the variance of the weights by more than 17 times.
>
> ```{r}
> sqrt(vif_log_weight) >2 
> ```
>
> -   The high multicollinearity implies that "log weight" shares more than half of its variance with other independent variables.
>
> ### b. Use Stepwise VIF Selection to remove highly collinear predictors.
>
> -   ***i.*** Use `vif(regr_log)` to compute VIF.
>
>     ```{r}
>     vif_log_cars <- vif(regr_log)
>     ```
>
> -   ***ii.*** Eliminate from your model the single independent variable with the largest VIF score that is also greater than 5.
>
>     ```{r message=FALSE, warning=FALSE}
>     sort(vif_log_cars[,'GVIF'], decreasing=TRUE) # generalized VIF
>     # displacement should be removed
>     multicollinearity <- function(model, data){
>       LM <- lm(model, data)
>       sort_order <- sort(vif(LM)[, 'GVIF'], decreasing=TRUE)
>       if (unname(sort_order[1] >5 )==TRUE){
>         print('Variable you should remove next: ')
>         names(sort_order)[1]
>       }else{
>         print('No more vif of variable is larger than 5.')
>         return(LM)
>       }
>     >
>     }
>     >
>     multicollinearity(mpg~cylinders+
>                         displacement+
>                         horsepower+
>                         weight+
>                         acceleration+
>                         cars$model_year+
>                         factor(origin),
>                       cars_log)
>     ```
>
> -   ***iii.*** Repeat steps (i) and (ii)
>
>     ```{r message=FALSE, warning=FALSE}
>     multicollinearity(mpg~cylinders+
>                         horsepower+
>                         weight+
>                         acceleration+
>                         cars$model_year+
>                         factor(origin), cars_log)
>     >
>     multicollinearity(mpg~cylinders+
>                         weight+
>                         acceleration+
>                         cars$model_year+
>                         factor(origin), cars_log)
>     >
>     multicollinearity(mpg~weight+
>                         acceleration+
>                         cars$model_year+
>                         factor(origin), cars_log)
>     final_Regression_model <- multicollinearity(mpg~weight+
>                                                   acceleration+
>                                                   cars$model_year+
>                                                   factor(origin), cars_log)
>     ```
>
> -   ***iv.*** Report the final regression model and its summary statistics.
>
>     ```{r message=FALSE, warning=FALSE}
>     summary(final_Regression_model)
>     # to compare
>     summary(LMOfCars(cars_log))
>     ```
>
> ### c. Have we lost any variables that were previously significant? If so, how much did we hurt our explanation?
>
> ```{r message=FALSE, warning=FALSE}
> model_fit <- function(y, yhat){
>   variances <- list()
>   variances$SSE <- sum((yhat-y)^2)
>   variances$SSR <- sum((yhat-mean(y))^2)
>   variances$SST <- sum((y-mean(y))^2)
>   variances$Rsq <- sum((yhat-mean(y))^2)/sum((y-mean(y))^2)
>   return(variances)
> }
> >
> model_fit(cars_log$mpg, regr_log$fitted.values)$Rsq
> model_fit(cars_log$mpg, final_Regression_model$fitted.values)$Rsq
> >
> summary(regr_log)$r.squared-summary(final_Regression_model)$r.squared
> ```
>
> -   **Ans.** Yes, we lose horsepower and weight that were previously significant.
>
>     -   About 0.74%
>
> ### d. From only the formula for VIF...
>
> -   ***i.*** If an independent variable has no correlation with other independent variables, what would its VIF score be?
>
>     -   **Ans.** 1, when multicollinearity does not exist, standard error of an independent variable would not inflated.
>
> -   ***ii.*** Given a regression with only two independent variables (X1 and X2), how correlated would X1 and X2 have to be, to get VIF scores of 5 or higher? To get VIF scores of 10 or higher?
>
>     -   **Ans.**
>
>         $vif = [\frac{1}{(1-r^2)} > 5] = [r > \sqrt \frac{4}{5}]$
>
>         indicating that the coefficient of multiple correlation between dependent variable and independent variables should be greater that $80\%$
>
>         $vif = [\frac{1}{(1-r^2)} > 10] = [r > \sqrt \frac{9}{10}]$
>
>         This indicates that the coefficient of multiple correlation between dependent variable and independent variables should be greater that $90\%$

------------------------------------------------------------------------

## Question 3) Visualization

Might the relationship of weight on mpg be different for cars from different origins?

> ### a. Add three separate regression lines on the scatterplot.
>
> ```{r message=FALSE, warning=FALSE}
> origin_colors = c("blue", "darkgreen", "red")
> with(cars_log, plot(weight, mpg, pch=origin, col=origin_colors[origin]))
> >
> plt(cars_log$mpg, cars_log$weight, 'weight v.s. mpg seperated by orgin')
> >
> ggplot(cars_log, aes(weight, mpg, color=factor(origin)))+
>   geom_point(size=1.5)+
>   stat_smooth(method=lm)+
>   labs(color='Origin')+
>   theme(legend.position = 'bottom')+
>   ggtitle('Weight v.s. mpg seperated by origin')+
>   guides(color=guide_legend(override.aes = list(size=1.2)))
> >
> ```
>
> ### b. Do cars from different origins appear to have different weight vs. mpg relationships?
>
> **Ans.** I think so, yes.
